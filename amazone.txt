import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load model + tokenizer
tokenizer = AutoTokenizer.from_pretrained("LiYuan/amazon-review-sentiment-analysis")
model = AutoModelForSequenceClassification.from_pretrained("LiYuan/amazon-review-sentiment-analysis")

# Make sure model is in eval mode
model.eval()

# Function to get sentiment
def get_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        scores = torch.nn.functional.softmax(outputs.logits, dim=1)
        predicted_class = scores.argmax(dim=1).item()
    return predicted_class  # returns class index (e.g. 0–4 for 1 to 5 stars)

# Example: apply on cleanDF
# Assuming cleanDF['review_title'] and cleanDF['review_content'] exist
cleanDF["full_review"] = cleanDF["review_title"].fillna("") + " " + cleanDF["review_content"].fillna("")
cleanDF["sentiment"] = cleanDF["full_review"].apply(get_sentiment)

# If you want actual star ratings (1–5 instead of 0–4)
cleanDF["sentiment"] = cleanDF["sentiment"] + 1



# Install deps if needed:
# pip install faiss-cpu sentence-transformers ollama pandas

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import ollama

# -----------------------------
# 1. Load and prepare your reviews
# -----------------------------
df = pd.read_csv("amazon_reviews.csv")  
# Expect columns like: product_id, review_text, rating, title
df = df.dropna(subset=["review_text"])

# Optional: chunk reviews into smaller passages (2-3 sentences each)
import re
def chunk_text(text, max_words=60):
    words = text.split()
    for i in range(0, len(words), max_words):
        yield " ".join(words[i:i+max_words])

chunks = []
meta = []
for _, row in df.iterrows():
    for chunk in chunk_text(row["review_text"]):
        chunks.append(chunk)
        meta.append({"product_id": row["product_id"], "rating": row["rating"]})

# -----------------------------
# 2. Build embeddings
# -----------------------------
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")  
embeddings = embed_model.encode(chunks, convert_to_numpy=True)

# -----------------------------
# 3. Store in FAISS
# -----------------------------
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"Stored {len(chunks)} review chunks in FAISS index.")

# -----------------------------
# 4. Query function
# -----------------------------
def search_reviews(query, k=5):
    q_emb = embed_model.encode([query], convert_to_numpy=True)
    scores, idx = index.search(q_emb, k)
    results = [chunks[i] for i in idx[0]]
    return results

def answer_query(query, product_id=None):
    # Retrieve context
    retrieved = search_reviews(query, k=8)

    # Build system prompt
    context = "\n".join(retrieved)
    prompt = f"""
    You are a helpful assistant that answers questions based only on the provided reviews.

    Question: {query}

    Reviews:
    {context}

    Answer clearly and concisely, citing only what is in the reviews.
    If nothing relevant is found, say "The reviews don't provide enough information."
    """

    # Send to local Llama 3.1 (using Ollama)
    response = ollama.chat(model="llama3.1", messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# -----------------------------
# 5. Example usage
# -----------------------------
print(answer_query("What do customers say about the battery life of Product X?"))








      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for vllm
Failed to build vllm
error: failed-wheel-build-for-install

× Failed to build installable wheels for some pyproject.toml based projects




pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install Transformers
pip install --upgrade transformers accelerate

# Optional: for fast tokenization
pip install sentencepiece





from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,   # use bfloat16 or float16
    device_map="auto"            # automatically puts layers on available GPUs
)

prompt = "Explain quantum computing in simple terms."
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))


https://nlp.stanford.edu/projects/glove/




import numpy as np

def load_glove(file_path):
    embeddings = {}
    with open(file_path, 'r', encoding='utf8') as f:
        for line in f:
            values = line.strip().split()
            word = values[0]
            vector = np.array(values[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings

glove_path = "path_to_glove/glove.6B.100d.txt"
glove_embeddings = load_glove(glove_path)




import re

def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())  # tokenize
    vectors = [embeddings[word] for word in words if word in embeddings]
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)

# Apply to each column
for col in prossedDF.columns:
    prossedDF[col + "_vec"] = prossedDF[col].apply(lambda x: text_to_vector(x, glove_embeddings))

# Optional: combine all columns into one row vector
def combine_row_vectors(row):
    vectors = [row[col + "_vec"] for col in prossedDF.columns if col + "_vec" in row]
    return np.mean(vectors, axis=0)

prossedDF["row_vec"] = prossedDF.apply(combine_row_vectors, axis=1)






import re
import numpy as np
import torchtext.vocab as vocab

# Load GloVe 100d embeddings (will auto-download first time)
glove = vocab.GloVe(name="6B", dim=100)

# Function: convert text into vector by averaging word embeddings
def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())  # tokenize words
    vectors = [embeddings[word].numpy() for word in words if word in embeddings.stoi]  
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)

# Apply to each text column
for col in prossedDF.columns:
    prossedDF[col + "_vec"] = prossedDF[col].apply(lambda x: text_to_vector(x, glove, 100))

# Optional: combine vectors from all columns into a single row vector
def combine_row_vectors(row):
    vectors = [row[col + "_vec"] for col in prossedDF.columns if col + "_vec" in row]
    return np.mean(vectors, axis=0)

prossedDF["row_vec"] = prossedDF.apply(combine_row_vectors, axis=1)