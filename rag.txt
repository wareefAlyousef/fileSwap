# chunk reviews into smaller passages (2-3 sentences each)
import re
def chunk_text(text, max_words=60):
    words = text.split()
    for i in range(0, len(words), max_words):
        yield " ".join(words[i:i+max_words])

chunks = []
meta = []
for _, row in processedDF.iterrows():
    for chunk in chunk_text(row["review_text"]):
        chunks.append(chunk)
        meta.append({"product_id": row["product_id"], "rating": row["rating"]})



import re
import numpy as np
import torchtext.vocab as vocab

# Load GloVe 100d embeddings (will auto-download first time)
glove = vocab.GloVe(name="6B", dim=100)

# Function: convert text into vector by averaging word embeddings
def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())  # tokenize words
    vectors = [embeddings[word].numpy() for word in words if word in embeddings.stoi]  
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)

# Apply to each text column
for col in prossedDF.columns:
    prossedDF[col + "_vec"] = prossedDF[col].apply(lambda x: text_to_vector(x, glove, 100))

# Optional: combine vectors from all columns into a single row vector
def combine_row_vectors(row):
    vectors = [row[col + "_vec"] for col in prossedDF.columns if col + "_vec" in row]
    return np.mean(vectors, axis=0)

prossedDF["row_vec"] = prossedDF.apply(combine_row_vectors, axis=1)




def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())  # tokenize
    vectors = [embeddings[word] for word in words if word in embeddings]
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)


# Apply to each column
for col in processedDF.columns:
    processedDF[col + "_vec"] = processedDF[col].apply(lambda x: text_to_vector(x, glove_embeddings))

# Optional: combine all columns into one row vector
def combine_row_vectors(row):
    vectors = [row[col + "_vec"] for col in processedDF.columns if col + "_vec" in row]
    return np.mean(vectors, axis=0)

processedDF["row_vec"] = processedDF.apply(combine_row_vectors, axis=1)



# 3. Store in FAISS
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"Stored {len(chunks)} review chunks in FAISS index.")


# 4. Query function
def search_reviews(query, k=5):
    q_emb = embed_model.encode([query], convert_to_numpy=True)
    scores, idx = index.search(q_emb, k)
    results = [chunks[i] for i in idx[0]]
    return results

def answer_query(query, product_id=None):
    # Retrieve context
    retrieved = search_reviews(query, k=8)

    # Build system prompt
    context = "\n".join(retrieved)
    prompt = f"""
    You are a helpful assistant that answers questions based only on the provided reviews.

    Question: {query}

    Reviews:
    {context}

    Answer clearly and concisely, citing only what is in the reviews.
    If nothing relevant is found, say "The reviews don't provide enough information."
    """

    # Send to local Llama 3.1 (using Ollama)
    response = ollama.chat(model="llama3.1", messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# 5. Example usage
print(answer_query("What do customers say about the battery life of Product X?"))












import re
import numpy as np
import pandas as pd
import faiss
import torchtext.vocab as vocab
from groq import Groq

# -------------------
# 1. Chunk reviews
# -------------------
def chunk_text(text, max_words=60):
    words = str(text).split()
    for i in range(0, len(words), max_words):
        yield " ".join(words[i:i+max_words])

chunks = []
meta = []
for _, row in processedDF.iterrows():
    # merge title + content
    review_text = str(row["review_title"]) + " " + str(row["review_content"])
    for chunk in chunk_text(review_text):
        chunks.append(chunk)
        meta.append({"product_id": row["product_id"], "rating": row["rating"]})

# -------------------
# 2. GloVe embeddings
# -------------------
glove = vocab.GloVe(name="6B", dim=100)

def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())
    vectors = [embeddings[word].numpy() for word in words if word in embeddings.stoi]
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)

# Create embeddings for chunks
chunk_embeddings = np.array([text_to_vector(ch, glove, 100) for ch in chunks]).astype("float32")

# -------------------
# 3. Store in FAISS
# -------------------
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings)

print(f"Stored {len(chunks)} review chunks in FAISS index.")

# -------------------
# 4. Query function
# -------------------
def search_reviews(query, k=5):
    q_emb = np.array([text_to_vector(query, glove, 100)]).astype("float32")
    scores, idx = index.search(q_emb, k)
    results = [chunks[i] for i in idx[0]]
    return results

# -------------------
# 5. Answer query with Groq Llama 3.3
# -------------------
client = Groq(api_key="YOUR_GROQ_API_KEY")

def answer_query(query, product_id=None):
    retrieved = search_reviews(query, k=8)
    context = "\n".join(retrieved)

    prompt = f"""
You are a helpful assistant that answers questions based only on the provided reviews.

Question: {query}

Reviews:
{context}

Answer clearly and concisely, citing only what is in the reviews.
If nothing relevant is found, say "The reviews don't provide enough information."
"""

    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    return response.choices[0].message.content

# -------------------
# 6. Example usage
# -------------------
print(answer_query("What do customers say about the charging speed of this cable?"))





# -------------------
# 1. Prepare combined documents from all columns
# -------------------
def build_document(row):
    parts = []
    parts.append(f"Product: {row['product_name']}")
    parts.append(f"Category: {row['category']}")
    parts.append(f"Price: discounted {row['discounted_price']}, actual {row['actual_price']} (discount {row['discount_percentage']})")
    parts.append(f"Overall rating: {row['rating']} based on {row['rating_count']} reviews")
    parts.append(f"About product: {row['about_product']}")
    parts.append(f"User: {row['user_name']} (sentiment: {row['sentemnt']})")
    parts.append(f"Review title: {row['review_title']}")
    parts.append(f"Review content: {row['review_content']}")
    return " | ".join(map(str, parts))

# -------------------
# 2. Chunking
# -------------------
def chunk_text(text, max_words=80):
    words = str(text).split()
    for i in range(0, len(words), max_words):
        yield " ".join(words[i:i+max_words])

chunks, meta = [], []
for _, row in processedDF.iterrows():
    document = build_document(row)
    for chunk in chunk_text(document):
        chunks.append(chunk)
        meta.append({
            "product_id": row["product_id"],
            "product_name": row["product_name"],
            "category": row["category"],
            "rating": row["rating"],
            "sentiment": row["sentemnt"],
            "discounted_price": row["discounted_price"],
            "actual_price": row["actual_price"],
            "discount_percentage": row["discount_percentage"],
            "user_name": row["user_name"],
            "img_link": row["img_link"],
            "product_link": row["product_link"],
            "review_id": row["review_id"]
        })

# -------------------
# 3. Embedding with GloVe
# -------------------
glove = vocab.GloVe(name="6B", dim=100)

def text_to_vector(text, embeddings, dim=100):
    words = re.findall(r'\w+', str(text).lower())
    vectors = [embeddings[word].numpy() for word in words if word in embeddings.stoi]
    if len(vectors) == 0:
        return np.zeros(dim)
    return np.mean(vectors, axis=0)

chunk_embeddings = np.array([text_to_vector(ch, glove, 100) for ch in chunks]).astype("float32")

# -------------------
# 4. Store in FAISS
# -------------------
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings)

print(f"Stored {len(chunks)} review chunks with metadata in FAISS.")

# -------------------
# 5. Search function
# -------------------
def search_reviews(query, k=5, product_id=None, category=None):
    q_emb = np.array([text_to_vector(query, glove, 100)]).astype("float32")
    scores, idx = index.search(q_emb, k*2)  # retrieve more, then filter

    results = []
    for i in idx[0]:
        if i == -1: 
            continue
        entry = meta[i]
        if product_id and entry["product_id"] != product_id:
            continue
        if category and category not in str(entry["category"]):
            continue
        results.append((chunks[i], entry))
        if len(results) >= k:
            break
    return results

# -------------------
# 6. Groq Llama 3.3 Answer function
# -------------------
client = Groq(api_key="YOUR_GROQ_API_KEY")

def answer_query(query, product_id=None, category=None):
    retrieved = search_reviews(query, k=8, product_id=product_id, category=category)
    context = "\n\n".join([f"Chunk: {txt}\nMeta: {m}" for txt, m in retrieved])

    prompt = f"""
You are a helpful assistant that answers questions based only on the provided Amazon reviews and metadata.

Question: {query}

Context:
{context}

Answer clearly and concisely, citing only what is in the reviews/metadata.
If nothing relevant is found, say "The reviews don't provide enough information."
"""

    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    return response.choices[0].message.content

# -------------------
# 7. Example usage
# -------------------
print(answer_query("Do users mention fast charging?", product_id="B07F1P8KNV"))